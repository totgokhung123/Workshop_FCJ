trong quá trình tạo SQS thêm policy của S3 video vào: (videosense-upload-queue)
{
  "Version": "2012-10-17",
  "Id": "__default_policy_ID",
  "Statement": [
    {
      "Sid": "__owner_statement",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::905418067993:root"
      },
      "Action": "SQS:*",
      "Resource": "arn:aws:sqs:ap-southeast-1:905418067993:videosense-upload-queue"
    },
    {
      "Sid": "AllowS3SendMessage",
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "SQS:SendMessage",
      "Resource": "arn:aws:sqs:ap-southeast-1:905418067993:videosense-upload-queue",
      "Condition": {
        "ArnEquals": {
          "aws:SourceArn": "arn:aws:s3:::<tên-bucket-của-bạn>"
        }
      }
    }
  ]
}
-------------------------------------
Tạo enviroment cho lambda lấy S3 (Presigner) :
Mở AWS Console → Lambda → chọn function bạn đang dùng (chắc là presigner của bạn).

Vào tab Configuration → chọn Environment variables.

Click Edit → Add environment variable:

Key: BUCKET_NAME

Value: tên-bucket-của-bạn (ví dụ totgo1)

Click Save.
 code lambda:
# lambda_presigner_safe.py
import json
import os
import time
import logging
import boto3

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client('s3')

def lambda_handler(event, context):
    logger.info("Event received: %s", event)

    bucket = os.environ.get('BUCKET_NAME')
    if not bucket or not isinstance(bucket, str) or bucket.strip() == "":
        logger.error("Missing or invalid BUCKET_NAME environment variable.")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": "Missing BUCKET_NAME env var", "detail": "Set environment variable BUCKET_NAME to your S3 bucket name."})
        }

    # Lấy filename an toàn
    filename = None
    try:
        # HTTP API (apigatewayv2) truyền query params vào event['queryStringParameters']
        q = event.get('queryStringParameters') if isinstance(event, dict) else None
        if q and isinstance(q, dict):
            filename = q.get('filename')
    except Exception as e:
        logger.warning("Cannot read queryStringParameters: %s", e)

    # Nếu không có filename, tạo tên mặc định
    if not filename or not isinstance(filename, str):
        filename = f"uploads/{int(time.time())}.mp4"
    else:
        filename = filename if filename.startswith('uploads/') else f"uploads/{filename}"

    logger.info("Using bucket=%s key=%s", bucket, filename)

    try:
        url = s3.generate_presigned_url(
            ClientMethod='put_object',
            Params={'Bucket': bucket, 'Key': filename, 'ContentType': 'video/mp4'},
            ExpiresIn=3600
        )
    except Exception as e:
        logger.exception("Failed to create presigned URL")
        return {
            "statusCode": 500,
            "body": json.dumps({"error": "Failed to create presigned URL", "detail": str(e)})
        }

    return {
        "statusCode": 200,
        "headers": {"Content-Type": "application/json"},
        "body": json.dumps({"upload_url": url, "key": filename})
    }

test code lambda:
{
  "queryStringParameters": {"filename": "test123.mp4"}
}

tạo API generate-upload-url
tạo phương thức GET, POST

Cách gắn policy (Console)
Vào AWS Console → IAM → Roles.

Tìm role Presigner-role-vq5i02ad (của lambda bạn tạo).

Chọn tab Permissions → Add inline policy (hoặc Attach policies directly nếu tạo managed).

Chọn tab JSON, dán policy trên, Review policy, đặt tên (ví dụ AllowS3PutUploads) → Create policy.
code:
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "AllowPutUploadPrefix",
			"Effect": "Allow",
			"Action": [
				"s3:PutObject",
				"s3:PutObjectAcl"
			],
			"Resource": "arn:aws:s3:::totgo1/uploads/*"
		}
	]
}
---------------------------------------------------
#phương thức GET kiểm tra:
$response = Invoke-RestMethod -Uri "https://lx7tj1p0jg.execute-api.ap-southeast-1.amazonaws.com/generate-upload-url?filename=test1234.mp4" -Method GET
$uploadUrl = $response.upload_url
# Hiện đầy đủ URL (không bị ...):
Write-Host $uploadUrl
# hoặc code python :
import requests
r = requests.get("https://lx7tj1p0jg.execute-api.ap-southeast-1.amazonaws.com/generate-upload-url?filename=test1234.mp4")
upload_url = r.json()['upload_url']
print(upload_url)

with open(r"E:\AWS_FCJ\Hugo\video\test1234.mp4","rb") as f:
    r2 = requests.put(upload_url, data=f, headers={"Content-Type":"video/mp4"})
print(r2.status_code, r2.text)
(Nếu báo 200 thì thành công)
-------------------------------------------------
#kiểm tra S3, và SQS
aws s3api head-object --bucket totgo1 --key uploads/test1234.mp4 --region ap-southeast-1
aws sqs receive-message --queue-url "https://sqs.ap-southeast-1.amazonaws.com/905418067993/videosense-upload-queue" --max-number-of-messages 1 --region ap-southeast-1
---------------------------------------------
# tạo thư mục chứa thư viện cần thiết trong AWS Cloudshelll
mkdir -p ~/layer_build && cd ~/layer_build
rm -rf python
mkdir python
python3 -m pip install --no-cache-dir scenedetect[opencv-headless] -t python/
zip -r9 scenedetect-layer.zip python
ls -lh scenedetect-layer.zip
# tạo S3 "scenedetect-layer" chứa scenedetect-layer.zip file thư viện
# upload
aws s3 cp scenedetect-layer.zip s3://scenedetect-layer/scenedetect-layer.zip
#tạo layer trong lambda 
Chọn Layers
Chọn create layer, đặt tên scenedetect-layer
chọn Upload a file from Amazon S3
copy link S3 scenedetect-layer "https://scenedetect-layer.s3.ap-southeast-1.amazonaws.com/scenedetect-layer.zip"
Compatible runtimes chọn "python 3.9" và Compatible architectures chọn "x86_64"
Chọn Create
# tạo lambda "videosense-frame-extractor" "python 3.9"
sửa policy IAM của lambda và đặt tên "videosense-frame-extractor":
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Sid": "AllowReadUploads",
			"Effect": "Allow",
			"Action": [
				"s3:GetObject",
				"s3:GetObjectAcl"
			],
			"Resource": "arn:aws:s3:::totgo1/uploads/*"
		},
		{
			"Sid": "AllowWriteFrames",
			"Effect": "Allow",
			"Action": [
				"s3:PutObject",
				"s3:PutObjectAcl"
			],
			"Resource": "arn:aws:s3:::totgo1/frames/*"
		},
		{
			"Sid": "DynamoDBVideosMetadata",
			"Effect": "Allow",
			"Action": [
				"dynamodb:GetItem",
				"dynamodb:PutItem",
				"dynamodb:UpdateItem"
			],
			"Resource": "arn:aws:dynamodb:ap-southeast-1:905418067993:table/videos"
		},
		{
			"Sid": "CWLogs",
			"Effect": "Allow",
			"Action": [
				"logs:CreateLogGroup",
				"logs:CreateLogStream",
				"logs:PutLogEvents"
			],
			"Resource": "arn:aws:logs:ap-southeast-1:905418067993:*"
		},
		{
			"Effect": "Allow",
			"Action": [
				"sqs:ReceiveMessage",
				"sqs:DeleteMessage",
				"sqs:GetQueueAttributes"
			],
			"Resource": "arn:aws:sqs:ap-southeast-1:905418067993:videosense-upload-queue"
		}
	]
}

# tạo DynamoDB :
#Tạo DynamoDB table "videos"
- Mở DynamoDB
- Click Create table. Table name: "videos" Partition key: video_id → Type: String
(Không cần sort key nếu bạn chỉ lưu metadata video. Nếu muốn lưu từng frame làm item riêng, ta có thể thêm frame_number sau.)
- Click Create table.
Khi table tạo xong, bạn sẽ thấy videos trong danh sách.

#Mở Lambda function videosense-frame-extractor → Configuration:
Environment variables:
BUCKET_NAME = totgo1
VIDEOS_TABLE = videos
MAX_FRAMES = 300 (tùy bạn)
EXTRACT_FPS = 30
TMP_DIR = /tmp
LOG_LEVEL = INFO
#Triggers: xác nhận SQS trigger đã bật (batch size = 1).
# add layer 
Chọn Add layer
chọn Custom layer -> chọn layer đã tạo "scenedetect-layer"
# code lambda test video processing với scenedetect
# app.py
import os
import decimal
import json
import time
import hashlib
import logging
import tempfile
import shutil
from urllib.parse import unquote_plus

import boto3
import botocore
import cv2
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector

# Setup
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=LOG_LEVEL)
logger = logging.getLogger("videosense-frame-extractor")

s3 = boto3.client("s3")
dynamodb = boto3.resource("dynamodb")

BUCKET = os.getenv("BUCKET_NAME", "totgo1")
VIDEOS_TABLE_NAME = os.getenv("VIDEOS_TABLE", "videos")
MAX_FRAMES = int(os.getenv("MAX_FRAMES", "100"))
EXTRACT_FPS = int(os.getenv("EXTRACT_FPS", "30"))
TMP_DIR = os.getenv("TMP_DIR", "/tmp")

table = dynamodb.Table(VIDEOS_TABLE_NAME)

def parse_sqs_record(record):
    """
    Parse an SQS record and return (bucket, key).
    Supports body being S3 event JSON (string) or direct JSON with 'bucket'/'key'.
    """
    body = record.get("body")
    if not body:
        raise ValueError("Empty SQS record body")

    # body might be JSON string
    try:
        payload = json.loads(body)
    except Exception:
        payload = body

    # If payload is S3 event structure
    if isinstance(payload, dict) and payload.get("Records"):
        try:
            s3info = payload["Records"][0]["s3"]
            bucket = s3info["bucket"]["name"]
            key = unquote_plus(s3info["object"]["key"])
            return bucket, key
        except Exception:
            pass

    # If payload is dict with bucket/key
    if isinstance(payload, dict) and payload.get("bucket") and payload.get("key"):
        return payload["bucket"], unquote_plus(payload["key"])

    raise ValueError("Cannot parse SQS body to get S3 bucket/key")

def make_deterministic_video_id(bucket, key):
    """
    Use S3 object's ETag if available to produce deterministic id.
    """
    try:
        head = s3.head_object(Bucket=bucket, Key=key)
        etag = head.get("ETag", "").strip('"')
    except botocore.exceptions.ClientError as e:
        logger.warning("head_object failed: %s", e)
        etag = None

    if etag:
        raw = f"{bucket}|{key}|{etag}"
    else:
        # fallback: use timestamp so not strictly deterministic but unique
        raw = f"{bucket}|{key}|{int(time.time())}"
    vid_hash = hashlib.sha256(raw.encode("utf-8")).hexdigest()
    return vid_hash

def write_status(video_id, status, extra=None):
    item = {
        "video_id": video_id,
        "status": status,
        "updated_at": int(time.time())
    }
    if extra and isinstance(extra, dict):
        item.update(extra)
    table.put_item(Item=item)
    logger.info("Wrote status=%s for video_id=%s", status, video_id)

def detect_scenes_local(video_path, downscale_factor=4, threshold=30.0):
    """
    Detect scenes and return scene_list.
    This function is robust to different PySceneDetect API signatures.
    """
    vm = VideoManager([video_path])
    sm = SceneManager()
    sm.add_detector(ContentDetector(threshold=threshold))

    try:
        # optional speed up
        try:
            vm.set_downscale_factor(downscale_factor)
        except Exception:
            # older/newer API might not have this method; ignore if not present
            pass

        # Start the video manager (required)
        vm.start()

        # Try calling detect_scenes using different possible signatures.
        # 1) common: sm.detect_scenes(frame_source=vm)
        # 2) fallback: sm.detect_scenes(vm)
        # 3) final fallback: try scenedetect.detect_scenes(vm) if available
        scene_list = None
        try:
            sm.detect_scenes(frame_source=vm)
        except TypeError:
            try:
                sm.detect_scenes(vm)
            except TypeError:
                # As a last attempt, try the module-level helper if present
                try:
                    from scenedetect import detect_scenes as sd_detect
                    sd_detect(vm, sm)
                except Exception as e:
                    # If all fails, re-raise for higher-level handling
                    raise RuntimeError("detect_scenes call failed for all known signatures") from e

        # get base_timecode if available for precise scene_list
        try:
            base_time = vm.get_base_timecode()
            scene_list = sm.get_scene_list(base_time)
        except Exception:
            # fallback to simpler getter
            try:
                scene_list = sm.get_scene_list()
            except Exception:
                # if still failing, return empty list
                scene_list = []

        return scene_list

    finally:
        try:
            vm.release()
        except Exception:
            pass

def extract_frames_upload(video_path, video_id, desired_fps=30, max_frames=300):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError("Cannot open video file for reading")

    video_fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    duration = total_frames / video_fps if video_fps else 0.0
    logger.info("Video opened: fps=%s total_frames=%s duration=%s", video_fps, total_frames, duration)

    # step in source frames for approx desired_fps
    step = max(1, int(round(video_fps / float(desired_fps)))) if desired_fps > 0 else 1
    logger.info("Extract step: take 1 frame every %s source frames", step)

    uploaded = 0
    idx = 0
    frame_no = 0

    # ensure tmp frames dir
    frames_tmp_dir = os.path.join(TMP_DIR, f"frames_{video_id}")
    os.makedirs(frames_tmp_dir, exist_ok=True)

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            if frame_no % step == 0:
                # build local filename
                fname = f"frame_{idx:06d}.webp"
                local_path = os.path.join(frames_tmp_dir, fname)
                # write webp with quality ~80
                ok = cv2.imwrite(local_path, frame, [int(cv2.IMWRITE_WEBP_QUALITY), 80])
                if not ok:
                    logger.warning("cv2.imwrite failed for %s", local_path)
                # upload to S3
                s3_key = f"frames/{video_id}/{fname}"
                try:
                    s3.upload_file(local_path, BUCKET, s3_key)
                except Exception as e:
                    logger.exception("Failed upload %s to s3://%s/%s", local_path, BUCKET, s3_key)
                    raise
                # remove local file to save space
                try:
                    os.remove(local_path)
                except Exception:
                    pass
                uploaded += 1
                idx += 1
                if uploaded >= max_frames:
                    logger.info("Reached MAX_FRAMES %s", max_frames)
                    break
            frame_no += 1
    finally:
        cap.release()
        # remove frames tmp dir if empty
        try:
            if os.path.isdir(frames_tmp_dir):
                shutil.rmtree(frames_tmp_dir)
        except Exception:
            pass

    return uploaded, total_frames, duration, video_fps

def lambda_handler(event, context):
    logger.info("Lambda invoked; event keys=%s", list(event.keys()))
    start_ts = int(time.time())

    # Parse SQS
    try:
        record = event["Records"][0]
        bucket, key = parse_sqs_record(record)
    except Exception as e:
        logger.exception("Cannot parse SQS record: %s", e)
        raise

    logger.info("Processing s3://%s/%s", bucket, key)

    # Generate deterministic video_id
    video_id = make_deterministic_video_id(bucket, key)
    logger.info("video_id=%s", video_id)

    # Skip if already processed
    try:
        resp = table.get_item(Key={"video_id": video_id})
        if resp.get("Item", {}).get("status") == "processed":
            logger.info("Video %s already processed; skipping", video_id)
            return {"statusCode": 200, "body": f"already processed {video_id}"}
    except Exception as e:
        logger.warning("DynamoDB get_item failed: %s", e)

    # Mark start
    write_status(video_id, "processing", {
        "original_key": key,
        "processing_started_at": start_ts
    })

    local_video_path = os.path.join(TMP_DIR, os.path.basename(key))

    try:
        # Download video
        logger.info("Downloading from s3://%s/%s to %s", bucket, key, local_video_path)
        s3.download_file(bucket, key, local_video_path)

        # Detect scenes
        try:
            scene_list = detect_scenes_local(local_video_path)
        except Exception as e:
            logger.warning("Scene detection failed: %s", e)
            scene_list = []

        scenes_count = len(scene_list)
        logger.info("Detected %d scenes", scenes_count)
        for i, (start, end) in enumerate(scene_list[:10]):
            logger.info("Scene %d: %s -> %s", i, start, end)

        # Extract frames
        uploaded, total_frames, duration, video_fps = extract_frames_upload(
            local_video_path, video_id, desired_fps=EXTRACT_FPS, max_frames=MAX_FRAMES
        )
        logger.info("Uploaded %d frames (total=%s, duration=%.2fs, fps=%.2f)",
                    uploaded, total_frames, duration, video_fps)

        # Save metadata
        table.put_item(Item={
            "video_id": video_id,
            "original_key": key,
            "status": "processed",
            "frames_count": uploaded,
            "scenes_count": scenes_count,
            "frames_prefix": f"frames/{video_id}/",
            "processing_started_at": decimal.Decimal(start_ts),  # Convert to Decimal
            "processing_finished_at": decimal.Decimal(int(time.time())),  # Convert to Decimal
            "duration_seconds": decimal.Decimal(duration).quantize(decimal.Decimal('0.00')),  # Convert to Decimal with precision
            "total_frames": total_frames,  # Assuming this is an integer
            "video_fps": decimal.Decimal(video_fps).quantize(decimal.Decimal('0.00'))  # Convert to Decimal with precision
        })

    except Exception as e:
        logger.exception("Processing failed for %s", video_id)
        write_status(video_id, "failed", {
            "error": str(e),
            "processing_finished_at": int(time.time())
        })
        raise
    finally:
        # Cleanup local files
        for path in [local_video_path, os.path.join(TMP_DIR, f"frames_{video_id}")]:
            try:
                if os.path.exists(path):
                    if os.path.isfile(path):
                        os.remove(path)
                    else:
                        shutil.rmtree(path)
            except Exception as ce:
                logger.warning("Cleanup failed for %s: %s", path, ce)

    return {"statusCode": 200, "body": json.dumps({
        "video_id": video_id,
        "frames_uploaded": uploaded
    })}
-------------------------------------------------------------------------------------
# Test giai đoạn "Video processing"
# Upload video có 2 phương thức
Vào S3 -> Buckets "totgo1" -> vào folder "uploads/" -> Chọn "Upload" file mp4 bất kỳ
Hoặc dùng API gateway như test của giai đoạn "Video & uploaded":
import requests
r = requests.get("https://lx7tj1p0jg.execute-api.ap-southeast-1.amazonaws.com/generate-upload-url?filename=test1234.mp4")
upload_url = r.json()['upload_url']
print(upload_url)
with open(r"E:\AWS_FCJ\Hugo\video\test1234.mp4","rb") as f:
    r2 = requests.put(upload_url, data=f, headers={"Content-Type":"video/mp4"})
print(r2.status_code, r2.text)
#Kiểm tra sau khi upload xong lên S3 kiểm tra thông báo SQS
Vào SQS -> chọn "videosense-upload-queue" -> chọn "Monitoring" để kiểm tra các thông số.
Vào Lambda -> Functions -> "videosense-frame-extractor" -> Chọn Test
Trong Event name để test copy code vào Event JSON, sau rồi save và test:
{
  "Records": [
    {
      "body": "{\"Records\":[{\"s3\":{\"bucket\":{\"name\":\"totgo1\"},\"object\":{\"key\":\"uploads/test1234_2.mp4\"}}}]}"
    }
  ]
}
Sau khi "run" xong:
- kiểm tra DynamoDB 
Chọn DynamoDB -> chọn "videos" -> chọn "Monitor" -> kiểm tra "CloudWatch metrics" 
- kiểm tra S3 "totgo1"
Vào Bucket "totgo1" -> chọn folder "frames/" -> kiểm tra tất cả frame được trích xuất
- Kiểm tra 



